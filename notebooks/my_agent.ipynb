{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbaa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(BaseAgent):\n",
    "    \"\"\"A simple Q-learning agent for the sailing environment using only local information.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.9, exploration_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.np_random = np.random.default_rng()\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        \n",
    "        # State discretization parameters\n",
    "        self.position_bins = 8     # Discretize the grid into 8x8\n",
    "        self.velocity_bins = 4     # Discretize velocity into 4 bins\n",
    "        self.wind_bins = 8         # Discretize wind directions into 8 bins\n",
    "        \n",
    "        # Initialize Q-table\n",
    "        # State space: position_x, position_y, velocity_direction, wind_direction\n",
    "        # Action space: 9 possible actions\n",
    "        self.q_table = {}\n",
    "        \n",
    "    def discretize_state(self, observation):\n",
    "        \"\"\"Convert continuous observation to discrete state for Q-table lookup.\"\"\"\n",
    "        # Extract position, velocity and wind from observation\n",
    "        x, y = observation[0], observation[1]\n",
    "        vx, vy = observation[2], observation[3]\n",
    "        wx, wy = observation[4], observation[5]\n",
    "        \n",
    "        # Discretize position (assume 32x32 grid)\n",
    "        grid_size = 32\n",
    "        x_bin = min(int(x / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        y_bin = min(int(y / grid_size * self.position_bins), self.position_bins - 1)\n",
    "        \n",
    "        # Discretize velocity direction (ignoring magnitude for simplicity)\n",
    "        v_magnitude = np.sqrt(vx**2 + vy**2)\n",
    "        if v_magnitude < 0.1:  # If velocity is very small, consider it as a separate bin\n",
    "            v_bin = 0\n",
    "        else:\n",
    "            v_direction = np.arctan2(vy, vx)  # Range: [-pi, pi]\n",
    "            v_bin = int(((v_direction + np.pi) / (2 * np.pi) * (self.velocity_bins-1)) + 1) % self.velocity_bins\n",
    "        \n",
    "        # Discretize wind direction\n",
    "        wind_direction = np.arctan2(wy, wx)  # Range: [-pi, pi]\n",
    "        wind_bin = int(((wind_direction + np.pi) / (2 * np.pi) * self.wind_bins)) % self.wind_bins\n",
    "        \n",
    "        # Return discrete state tuple\n",
    "        return (x_bin, y_bin, v_bin, wind_bin)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        \"\"\"Choose an action using epsilon-greedy policy.\"\"\"\n",
    "        # Discretize the state\n",
    "        state = self.discretize_state(observation)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if self.np_random.random() < self.exploration_rate:\n",
    "            # Explore: choose a random action\n",
    "            return self.np_random.integers(0, 9)\n",
    "        else:\n",
    "            # Exploit: choose the best action according to Q-table\n",
    "            if state not in self.q_table:\n",
    "                # If state not in Q-table, initialize it\n",
    "                self.q_table[state] = np.zeros(9)\n",
    "            \n",
    "            # Return action with highest Q-value\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        \"\"\"Update Q-table based on observed transition.\"\"\"\n",
    "        # Initialize Q-values if states not in table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = np.zeros(9)\n",
    "        if next_state not in self.q_table:\n",
    "            self.q_table[next_state] = np.zeros(9)\n",
    "        \n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.discount_factor * self.q_table[next_state][best_next_action]\n",
    "        td_error = td_target - self.q_table[state][action]\n",
    "        self.q_table[state][action] += self.learning_rate * td_error\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the agent for a new episode.\"\"\"\n",
    "        # Nothing to reset for Q-learning agent\n",
    "        pass\n",
    "        \n",
    "    def seed(self, seed=None):\n",
    "        \"\"\"Set the random seed.\"\"\"\n",
    "        self.np_random = np.random.default_rng(seed)\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Save the Q-table to a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.q_table, f)\n",
    "            \n",
    "    def load(self, path):\n",
    "        \"\"\"Load the Q-table from a file.\"\"\"\n",
    "        import pickle\n",
    "        with open(path, 'rb') as f:\n",
    "            self.q_table = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
